%% LyX 2.1.3 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[10pt]{beamer}
\usepackage{etex}

\usepackage[T3,T1]{fontenc}
\usepackage{textpos} 
\usepackage{hyperref}
\usepackage{amsmath,amsthm,amsfonts,nicefrac,mathabx,amssymb,amsbsy,bbm}
\usepackage[subnum]{cases}
\usepackage{calligra, mathrsfs}
%\usepackage{natbib}
\usepackage{booktabs}
%\bibpunct{(}{)}{;}{a}{,}{,}
\usepackage[english]{babel}
\usepackage[latin1]{inputenc}
\usepackage{helvet}
\usepackage{graphicx}
\usepackage{color}
\usepackage{multirow,dcolumn}
\usepackage{listings}
\usepackage{ragged2e}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{animate} %for animated graphs
\usepackage{graphicx}

\usepackage{url}
\usepackage{bibentry}
\usepackage{chngcntr}

\ifx\hypersetup\undefined
  \AtBeginDocument{%
    \hypersetup{unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,
 breaklinks=false,pdfborder={0 0 0},backref=false,colorlinks=false}
  }
\else
  \hypersetup{unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,
 breaklinks=false,pdfborder={0 0 0},backref=false,colorlinks=false}
\fi
\DeclareSymbolFont{tipa}{T3}{cmr}{m}{n}
\DeclareMathAccent{\invbreve}{\mathalpha}{tipa}{16}


\colorlet{tablesubheadcolor}{gray!25}
\colorlet{tableheadcolor}{gray!40}
\colorlet{tablerowcolor}{gray!15.0}
\usetheme{Gesis}
%\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{footline}[frame number]%{\hspace*{.2cm}\insertframenumber}
\setbeamerfont{caption}{size=\footnotesize}
\usefonttheme[onlylarge]{structuresmallcapsserif} % alte Schrift


\definecolor{hellgrau}{rgb}   {0.109375,  0.40625,   0.51953125}
\definecolor{dunkelgrau}{rgb} {0.009375,  0.30625,   0.41953125}
\definecolor{dunkelgrau2}{rgb}{0.009375,  0.20625,   0.31953125}
\definecolor{hellbraun}{rgb}  {0.9140625, 0.8984375, 0.8046875}
\definecolor{hellbraun2}{rgb} {.95,       0.9,       0.8}
\definecolor{alertred}{rgb}   {0.8515625, 0.3828125, 0.08984375}
\definecolor{orange}{rgb}{1,0.5,0}


\setbeamercolor{firstsecslide}{fg=white,bg=dunkelgrau}
\setbeamertemplate{blocks}[rounded][shadow=true]

\newcolumntype{d}[1]{D{.}{.}{#1}}

\newcommand{\emphred}[1]{\textcolor{alertred}{#1}}
\newcommand{\emphcol}[1]{\textcolor{dunkelgrau}{\slshape #1}}

\newcommand{\eqname}[1]{\tag*{#1}} %equation title

\newenvironment{frcseries}{\fontfamily{frc}\selectfont}{}
\newcommand{\textfrc}[1]{{\frcseries#1}}
\newcommand{\mathfrc}[1]{\text{\textfrc{#1}}}


\setcounter{tocdepth}{1}
\setbeamercolor*{section in toc}{fg=hellgrau}
\setbeamertemplate{bibliography item}[default]


\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}

\newsavebox{\mysavebox}


\newcommand{\E}[1]{\text{E}\left(#1\right)}
\newcommand{\V}[1]{\text{V}\left(#1\right)}
\newcommand{\Vest}[1]{\widehat{\text{V}}\left(#1\right)}
\newcommand{\MSE}[1]{\text{MSE}\left(#1\right)}
\newcommand{\COV}[2]{\text{COV}\left(#1,\,#2\right)}
\newcommand{\deff}{\ensuremath{\text{\slshape deff}}}
%roman numbers in equation
\newcommand{\RN}[1]{%
  \textup{\uppercase\expandafter{\romannumeral#1}}%
}
\newcommand{\deffhat}{\ensuremath{\widehat{\text{\slshape deff}}}}
\newcommand{\deffc}{\ensuremath{\text{\slshape deff}_{\text{\slshape c}}}}
\newcommand{\deffhatc}{\ensuremath{\widehat{\text{\slshape deff}}_{\text{\slshape c}}}}
\newcommand{\deffp}{\ensuremath{\text{\slshape deff}_{\text{\slshape p}}}}
\newcommand{\neff}{\ensuremath{n_\text{\slshape eff}}}
\newcommand{\nnet}{\ensuremath{n_\text{\slshape net}}}
\newcommand{\ngross}{\ensuremath{n_\text{\slshape gross}}}


\makeatletter


\addtobeamertemplate{frametitle}{}{%
\begin{textblock*}{100mm}(.91\textwidth,-1cm)
\includegraphics[height=1cm,width=2cm]{graphs/logos/GESIS_Logo_kompakt_en.jpg}
\end{textblock*}}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
\providecommand{\LyX}{\texorpdfstring%
  {L\kern-.1667em\lower.25em\hbox{Y}\kern-.125emX\@}
  {LyX}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
% this default might be overridden by plain title style
 \newcommand\makebeamertitle{\frame{\maketitle}}%
 % (ERT) argument for the TOC
 \AtBeginDocument{%
   \let\origtableofcontents=\tableofcontents
   \def\tableofcontents{\@ifnextchar[{\origtableofcontents}{\gobbletableofcontents}}
   \def\gobbletableofcontents#1{\origtableofcontents}
 }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
%\usetheme{Montpellier}%\usetheme{PaloAlto}

\makeatother

\begin{document}
<<setup, include=FALSE>>=
library(PracTools) 
library(knitr)
library(xtable)
library(plyr)
library(survey)
library(sampling)
library(Matrix)
opts_chunk$set(fig.path='graphs/beamer-',fig.align='center',fig.show='hold',size='footnotesize')
@


\title[Variance Estimation]{4. GESIS Summer School in Survey Methodology
Cologne. Course 16: Sampling, Weighting and Estimation}  
\subtitle{Day 5: Resampling Methods for Variance Estimation}

\author{Stefan Zins\thanks{\href{mailto:Stefan.Zins@gesis.org}{Stefan.Zins@gesis.org}} and Matthias Sand\thanks{\href{mailto:Matthias.Sand@gesis.org}{Matthias.Sand@gesis.org}}}
\date{Friday, August 28, 2015} 

\makebeamertitle

\section{Resampling methods}
\begin{frame}{Resampling methods}

\begin{itemize}
\item Idea: draw repeatedly (sub-)samples from the sample in order to build the sampling distribution of the estimator.
\item Estimate the variance as variability of the estimates from the resamples.
\item Methods of interest
\begin{itemize}
\item Balanced repeated replication
\item Jackknife
\item Bootstrap
\end{itemize}
\item Some remarks:
\begin{itemize}
\item If it works, one does neither need the $\pi_{kl}$'s nor the derivatives (i.e. the linearized variable) of the estimator.
\item However they are difficult to adapt to complex sampling designs, especially to designs with unequal probability sampling.
\item They can be computational intense.
\end{itemize}
\end{itemize}
\end{frame}


% \begin{frame}{Random groups}
% \begin{itemize}
% \item Random partition of sample into $R$ groups (independently)
% \item $\hat{\theta}_{(r)}$ denotes the estimate of $\theta$ on $r$-th subsample
% \item Random group points estimate:
% \begin{gather*}
% \hat{\theta}_\text{RG}=\dfrac1R\cdot\sum\limits_{r=1}^R \hat{\theta}_{(r)}
% \end{gather*}
% \item Random group variance estimate:
% \begin{gather*}
% \Vest{\hat{\theta}}_\text{RG}=  \dfrac{1}{R(R-1)}  \sum\limits_{r=1}^R \left(\hat{\theta}_{(r)} - \widehat{\theta}_\text{RG}\right)^2
% \end{gather*}
% \end{itemize}
% \end{frame}
 
\begin{frame}[fragile]{The Balanced Repeated \\ Replication Method}
\onslide*<1>{Balanced repeated replication (BRR) is (only) suitable for stratified samples.  Half samples are build by selecting half the elements from each stratum.
\begin{itemize}
\item In the original version of the method there are only two elements per stratum.
\item $\hat{\theta}_{(r)}$ is the estimate of the $r$-th half sample.
\item For $H$ strata there are $2^H$ possible half samples, instead of applying them all we use a balanced selection $R \ll 2^H$ via Hadamard matrices $\boldsymbol{\Lambda}$. 
\end{itemize}
}

\onslide<2->{A Hadamar matrices is a square matrix whose entries are either +1 or -1 and whose rows and columns are mutually orthogonal. 
$$
\boldsymbol{\Lambda}=
\left(\begin{array}{rrrr}
<<hadamardMatrix,echo=FALSE, results='asis'>>=
   H<-survey::hadamard(3)
   print(xtable(H*2-1,digits = 0),only.contents=TRUE
         ,hline.after=NULL, include.rownames=FALSE,include.colnames=FALSE)
 @
\end{array}\right)
$$
The rows of a $R \times R$ Hadamard matrix denote the half samples and the columns the strata, where
$H+1 \leq R \leq H+4$, thus the half samples are drawn mutually independent.
$\lambda_{rh}$ is the element in $\boldsymbol{\Lambda}$ in the $r$-th row and the $h$-column. If $\lambda_{rh} = 1$ the first half of stratum $h$ is in the $r$-th replicate and for $\lambda_{rh} = -1$ the second. 
}
%with replicate weights
\end{frame}

\begin{frame}{BRR Replicate Weights I}
\onslide<1->{
For $n_{h} >2$ one possibility is to divide the PSU's in stratum $h$ randomly into two disjoint sets $\mathfrc{s}_{h_1}$ and $\mathfrc{s}_{h_2}$ of sizes $n_{h_1}=\lfloor n_{h}/2 \rfloor$ and $n_{h_2} = n_{h} - n_{h_1}$, respectively.}
\onslide<2->{
The weights of the $r$-th replicate have to be adjusted. For $k \in \mathfrc{s}_{h_1}$ we have:
\begin{gather*}
d_{k\,(r)}:=\left\{
\begin{array}{ll}
d_{k}  \left[1 + \left(\dfrac{n_{h_2}  \left(1- \frac{n_h}{N_h} \right)}{n_{h_1}}\right)^{1/2}\right] & \text{if}\; \lambda_{rh} = 1\\ 
d_{k}  \left[1 - \left(\dfrac{n_{h_1}  \left(1-\frac{n_h}{N_h}  \right)}{n_{h_2}}\right)^{1/2}\right] & \text{if}\; \lambda_{rh} = -1\\
\end{array}
\right.\,{.}
\end{gather*}
}
\onslide<3->{
The standard BRR variance estimator is given by
$$
\Vest{\hat{\theta}}_\text{BRR} = \dfrac1R \sum\limits_{r=1}^R \left( \widehat{\theta}_{(r)} - \widehat{\theta} \right)^2\;.
$$
}
\end{frame}

\begin{frame}{BRR Replicate Weights II}
There are alternative forms of BRR, that try to cope with the possible instability of the BRR variance estimator.
Fay (1989)  suggests  a  scheme  which  makes  the  weighting  milder by choosing a factor $0 < \epsilon \leq 1$
The resulting replicated weight for  $k \in \mathfrc{s}_{h_1}$  is
\begin{gather}
d_{k\,(r)}:=\left\{
\begin{array}{ll}
d_k \left[1 + \varepsilon  \left(\dfrac{n_{h_2} \cdot \left(1-\frac{n_h}{N_h} \right)}{n_{h_1}}\right)^{1/2}\right],& \lambda_{rh} = 1,\\
\\
d_k \left[1 - \varepsilon  \left(\dfrac{n_{h_1} \cdot \left(1-\frac{n_h}{N_h} \right)}{n_{h_2}}\right)^{1/2}\right],& \lambda_{rh} = -1.\\
\end{array}
\right.
\end{gather}

The resulting variance estimator is defined by
$$
\Vest{\hat{\theta}}_\text{GBRR}=
\dfrac{1}{R \varepsilon^2} \cdot \sum_{r=1}^{R}\left(\hat{\theta}_{(r),\varepsilon} - \hat{\theta} \right)^2,
$$
Setting $\epsilon = 1$ recovers weights from before.

\end{frame}

% \begin{frame}{Delete-1-Jackknife}
% %\begin{block}{Delete-1-Jackknife}
% \begin{itemize}
% \item Resampling by omitting (deleting) one element in each resample
% \item $\widehat{\theta}_{-i}$ is used in $n$ resamples
% \item Originally designed for bias estimation
% \end{itemize}
% %\end{block}
% \begin{block}{Bootstrap}
% \begin{itemize}
% \item Resampling by subsamples of size $n$
% \item Number of resamples $b$ is arbitrary
% \item WR \emph{only}
% \end{itemize}
% \end{block}
% \end{frame}

\begin{frame}{The Jackknife Method}
Originally, the Jackknife method  was introduced for estimating the bias of an estimator.
If $\hat\theta$ is our estimator then let
 $$
 \hat{\theta}_{-k} 
 $$
be its corresponding value based on sample $\mathfrc{s} \setminus \{k\}$.
% The delete-1-Jackknife (d1JK) bias for $\theta$ is
% \begin{gather*}
% \hat{\text{B}}_\text{d1JK}\left(\hat{\theta}\right) = (n-1)\cdot \left(
%     \frac1n\sum\limits_{k \in \mathfrc{s}}\widehat{\theta}_{-k} - \hat{\theta}\right)
% \end{gather*} [Shao and Tu, 1995].
% The jackknife estimator which reduces the bias can be written as
% $$\sum_{k \in \mathfrc{s}} \dfrac{\hat\theta_{(k)}}{n}\;{,}$$
% where $\hat\theta_{(k)}= n \hat\theta  - (n-1)\hat{\theta}_{-k}$.
\end{frame}

\begin{frame}{Jackknife variance estimation}
The replicate values $\hat{\theta}_{(k)}= n \hat\theta  - (n-1)\hat{\theta}_{-k}$ can be used to estimate the variance of $\hat\theta$.
This so called (delete-1) jackknife variance estimator is 
\begin{align*}
\Vest{\hat\theta}_\text{d1JK} & = \dfrac{1}{n(n-1)} \sum_{k\in\mathfrc{s}}\left(\hat{\theta}_{(k)} - \sum_{l \in \mathfrc{s} } \dfrac{\hat\theta_{(k)}}{n}  \right)^2 \\
    &= \dfrac{n-1}{n} \sum\limits_{k \in \mathfrc{s}} \left( \hat{\theta}_{-k} -  \sum_{l \in \mathfrc{s} } \dfrac{\hat\theta_{-l}}{n} \right)^2\;{,}
\end{align*}
[Shao and Tu, 1995].
\end{frame}
% 

\begin{frame}{The Jackknife Method with Stratification}
Also the jackknife can be implemented with replicated weights. For a stratified sample
if the $k$-th element in stratum $h$ is omitted, then the weights $d_l$ for element
$l$ in stratum $g$ also have to be recalculated:
$$
d_{(l\,,-k)} = \begin{cases}
 d_l & \text{if}\, g \neq h \\
 d_l \dfrac{n_h}{n_h-1} & \text{if}\, g = h,\, k \neq l \\
 0 & \text{if}\, g = h  ,\, k = l \\
\end{cases}
$$
$d_{(l\,,-k)}$ is  weight of element $l \in \mathcal{U}_g$ if element $k \in \mathcal{U}_h$ is deleted.
Then the delete-one jackknife estimate of variance is given by
$$
\Vest{\hat\theta}_\text{d1JK,StrRS}  = \sum_{h=1}^H \dfrac{(n_h-1)(1-\frac{n_h}{N_h})}{n_h}  \sum\limits_{k \in \mathfrc{s}_h} \left( \hat{\theta}_{h,\,-k} -  \sum_{l \in \mathfrc{s}_h } \dfrac{\hat\theta_{h,\,-l}}{n_h} \right)^2\;{,}
$$
where $\hat{\theta}_{h,\,-k}$ is the value of estimator $\hat\theta$ calculated without the $k \in \mathfrc{s}_h$ using the adjusted weights.
\end{frame}

%%
%%--------------------------------------------------

%%
\begin{frame}{Advantages and Disadvantages of the Jackknife}
\begin{itemize}
\item Very good for \emph{smooth} estimators.
\item Biased for the variance estimation of quantiles (e.g. median) and similar non-smooth statistics.
\item Specialized procedures are needed for complex designs, e.g. unequal probability sampling.
\item Huge effort in case of large samples sizes.
\item A possible application to non-smooth estimators can be achieved by delete-a-group jackknifes. 
\item Here the sample is divided into $G$ groups, which can be done randomly [Shao and Tu,  1995, p. 195] or systematically [Kott, 2001].
%\item Special corrections under imputations (cf.\ Chapter 4).
\end{itemize}
\end{frame}

% %%
% %%--------------------------------------------------
% %%

 
 

\begin{frame}{The Bootstrap Method I}
\begin{itemize}
\item<1-> Theoretical (parametric)  bootstrap: (Model based) The distribution of the variable of interest is estimated.
\item<2-> Monte-Carlo or empirical bootstrap: Random selection of size $n$ with replacement from the original sample.
% \begin{gather*}
% \Vest{\hat\theta}_\text{BootMC} = \dfrac{1}{B-1}\sum\limits_{i=1}^B \left( \hat{\theta}_{n,i}^*-\dfrac1B \sum\limits_{j=1}^B \hat{\theta}_{n,j}^* \right)^2\;{.}
% \end{gather*}
\item<3> Special adaptions are needed in complex surveys
\item<3> The method may have problem in case of large sampling fractions and sampling without replacement.
\end{itemize}
\end{frame}




\begin{frame}{The Theoretical Bootstrap}
If $F$ is the unknown distribution of your variable interest the bootstrap variance estimator results by substituting $F$ in the  theoretical formula of the variance define as
\begin{eqnarray}
    \V{\hat{\theta}}
        \nonumber  &=& \int\left[\hat{\theta} - \int\hat{\theta} d\prod\limits_{k=1}^{n}F(y_{k})\right]^2d\prod\limits_{k=1}^{n}F\left(y_{k}\right)
\end{eqnarray}
by  $\hat{F}$:

\begin{eqnarray}
    \nonumber \Vest{\hat{\theta}}_{\text{boot}}
    &=& \int\left[\hat{\theta} -
    \int\hat{\theta} d\prod\limits_{k=1}^{n}\hat{F}(y_{k})\right]^2d\prod\limits_{k=1}^{n}\hat{F}\left(y_{k}\right) \\
     \nonumber    &=& \V{\left[\hat{\theta}_{(r)}|\{ y_{k}| k \in \mathfrc{s} \} \right]}\;{,}
\end{eqnarray}
 where $\hat{\theta}_{(r)}$ is the value of $\hat\theta$ based on a sample drawn from $\hat{F}$, denoted as the
 \emph{bootstrap} sample. $\V{\left[ .|\{ y_{k}| k \in \mathfrc{s} \} \right]}$ is the conditional variance given the original sample data $\{ y_{k}| k \in \mathfrc{s} \}$.
 \end{frame}
% %%
% %%--------------------------------------------------
% %%


% %%
% %%--------------------------------------------------
% %%
\begin{frame}{Monte-Carlo-Bootstrap}
The theoretical bootstrap is often not practical to apply and a Monte-Carlo method is used to approximate it. $B$ resamples of size $n$ are drawn by SRSWR from the original sample.
Estimator $\hat\theta$ is calculated on these resamples. $\hat{\theta}_{(r)}$ denotes the value of $\hat\theta$ based on the $r$-rh resample.
Monte Carlo bootstrap variance estimator is given by
$$
\Vest{\hat{\theta}}_\text{MCboot} 
    =\dfrac{1}{B - 1} \sum_{r = 1}^{B}\left(\hat{\theta}_{(r)}-\dfrac{1}{B} \sum_{b =1}^{B}\hat{\theta}_{(b)} \right)^2\;{.}
$$
Due to the law of large numbers is $\Vest{\hat{\theta}}_{\text{boot}} = \lim \limits_{B\to\infty}
\Vest{\hat{\theta}}_\text{MCboot} $.
\end{frame}
% %%
% %%--------------------------------------------------
% %%


% %%
% %%--------------------------------------------------
% %%
\begin{frame}
\frametitle{Bootstrap Confidence Intervals}
Another application of the Monte Carlo bootstrap is to compute confidence intervals.
\begin{itemize}
\item<2-> Percentile Bootstrap
$$CI_{1-\alpha} = \left[ \hat\theta_{r,\alpha/2}\, {;}\, \hat\theta_{r,1-\alpha/2}  \right]$$
where  $\hat\theta_{r,1-\alpha/2}$  denotes the $1-\alpha/2$ percentile of the bootstrapped coefficients 
$\mathcal{B} = \{ \hat\theta_{(r)} )| r=1,\,,\ldots,\,B \}$.
\item<3-> Studentized Bootstrap 
$$CI_{1-\alpha} = \left[ \hat\theta - t_{r,(1-\alpha/2)}  \sqrt{\Vest{\hat{\theta}}_\text{MCboot}}; \hat\theta-t_{r,(\alpha/2)} \sqrt{\Vest{\hat{\theta}}_\text{MCboot}} \right]$$
 where $t_{r,(1-\alpha/2)}$ denotes the $1-\alpha/2$ percentile of the bootstrapped Student's t-test $t_{(r)}=\frac{ (\hat{\theta}_{(r)}-\hat{\theta})}{\sqrt{\Vest{\hat{\theta}}_\text{MCboot}}}$.
\end{itemize}
\uncover<4>{
The usage of bootstrap confidence intervals is also the reason why usually odd numbers of bootstrap replicates are used, like $B=49$ or $B=499$.
}
\end{frame}
% %%
% %%--------------------------------------------------
% %%
% 


\begin{frame}{Bootstrap with \\ Complex Survey Designs}
\onslide*<1>{
For multistage designs the bootstrap is applied at the first stage of the sampling design and when a PSU is drawn with replacement from the original sample all units of the following stages are also included in the bootstrap replicate [Wolter,\,2007,\, p.\, 211].
}
\onslide*<2->{
A further proposal of a particular bootstrap is the so called rescaling bootstrap. The approach can be described as follows:  From the $n_{h_\RN{1}}$ PSUs of the original sample in the $h$-th stratum $m_{h_\RN{1}}$ PSUs are drawn with replacement. The survey weight of SSU $k$ in the $i$-th PSU in the $h$-th stratum is for each resample adjusted to:
$$
d_{k,(r)}=\left[\left(1-\left(\dfrac{m_{h_\RN{1}}}{n_{h_\RN{1}}-1}\right)^{1/2}\right)+\left(\dfrac{m_{h_\RN{1}}}{n_{h_\RN{1}}-1}\right)^{1/2}  \left(\dfrac{n_{h_\RN{1}}}{m_{h_\RN{1}}}\right) r_{hi}\right] d_{k},
$$
where $r_{hi}$ describes the number of times a certain PSU appears in the resample. 
}
\onslide*<3>{
The value of $m_{h_\RN{1}}$ has to be determined. A choice with only a little, if any, loss in efficiency is $m_{h_\RN{1}}=(n_{h_\RN{1}}-1)$. Then the calculation of bootstrap weights reduces to:
$$ d_{k,(r)} =d_{k} \dfrac{n_{h_\RN{1}}}{\left(n_{h_\RN{1}}-1\right)} r_{hi}\;{.}$$
}
\end{frame}

\begin{frame}[fragile]{Resampling with the \texttt{survey} Package}
\begin{lrbox}{\mysavebox}
<<FunSStrRS>>=
strat.sample <- function(sind,n.h){
  N.h   <- table(sind)[names(n.h)] 
  N     <- length(sind)
  sam   <- mapply(function(x,y)sample(x,y)
                   ,x=N.h,y=n.h
                   ,SIMPLIFY = FALSE)
  sam   <- mapply(function(x,y)x[y]
                   ,x=split(1:N,sind)[names(n.h)],y=sam
                   ,SIMPLIFY = FALSE)
  as.numeric(1:N%in%unlist(sam))
}
@
\end{lrbox}
\onslide*<1>{ A function to select a simple stratified random sample
\usebox{\mysavebox} }

\begin{lrbox}{\mysavebox}
<<resamPrepareData,tidy=T>>=
library(survey)
#1. stage: districts are selected by a  simple StrRS;
#2. stage: schools are selected from sampled PSUs by a SRS

data(api)
## Make a stratification variable for the districts by grouping them by their mean value of 'api99'
apipop_d   <- apipop[!duplicated(apipop$dnum),]
api99.d    <- tapply(apipop$api99,apipop$dnum,mean)
apipop_d$sind   <- cut(api99.d,quantile(api99.d),include.lowest = T)

## make a new data set with design relevant information
apipop.       <- merge(apipop, apipop_d[,c("sind","dnum")], by="dnum")

#the finite population sizes of the PSU's  and SSU's
apipop.$fpc1  <- table(apipop_d$sind)[as.character(apipop.$sind)]
apipop.$fpc2  <- table(apipop.$dnum)[as.character(apipop.$dnum)]

#model for the a later calibration
pop.lm     <- lm(api00~sind:(stype+api99)-1+sind,data=apipop.)
pop.totals <- colSums(model.matrix(pop.lm))
@
\end{lrbox}
\onslide*<2>{ First we prepare the data set to select a two-stage sample
\usebox{\mysavebox} }

\begin{lrbox}{\mysavebox}
<<resamSample,echo=-1>>=
set.seed(1234)
n_I  <- 40 #select 40 PSU's
#proportional allocation of the 1. stage sample size
n_hI <- round(table(apipop_d$sind)/length(apipop_d$sind)*n_I)
##1. stage sample
s_I  <- strat.sample(sind=apipop_d$sind,n.h=n_hI)==1
#the sample of PSU's
apiclus1.str  <- apipop.[apipop.$dnum%in%apipop_d[s_I,"dnum"],]

#number of sampled SSU's in PSU's
N_i <- n_i <- table(apiclus1.str$dnum)
n_i[N_i<3]   <- N_i[N_i<3] #if less than 3 schools take all
n_i[N_i>=3]  <- 2          #and if their are more sample 2

##2. stage sample
s_i <- strat.sample(apiclus1.str$dnum,n.h=n_i)==1
#the final sample of schools
apiclus2.str <- apipop.[apipop.$snum%in%apiclus1.str[s_i,"snum"],] 

#define the 'svydesign' object and calibrate the weights
dclus2.str   <-  svydesign(id = ~dnum + snum, strata=~sind
                           ,fpc = ~fpc1 + fpc2, data = apiclus2.str)
cal.dclus2.str <- calibrate(dclus2.str,stage=0,population = pop.totals
                            ,formula = ~ sind:(stype+api99)+sind-1)
@
\end{lrbox}
\onslide*<3>{ Now we select the sample
\usebox{\mysavebox} }


\begin{lrbox}{\mysavebox}
<<resamSvyObs,echo=1:21,results='hide',warning=FALSE,cache=TRUE>>=
#BRR  weights
dclus2.Brr    <- as.svrepdesign(dclus2.str, type="BRR")
# BRR with Fay's weights
dclus2.FayBrr <- as.svrepdesign(dclus2.str, type="Fay",fay.rho=0.3)
# Jackknife for stratified designs
dclus2.Jkn    <- as.svrepdesign(dclus2.str, type="JKn")

#standard MC boostrap
dclus2.Boot <- as.svrepdesign(dclus2.str, type="bootstrap",replicates=99)

#MC sub-boostrap
dclus2.Subboot <- as.svrepdesign(dclus2.str, type="subbootstrap",replicates=99)

#all off the above resampling methods will produce warnings, because of the
#multistage design

##MC multistage boostrap after Preston (2009).
dclus2.Mrbboot<-as.svrepdesign(dclus2.str, type="mrbbootstrap",replicates=99)

#a matirx of the replicate weights
dclus2.Mrbboot$repweights

Vest.resam<-
as.matrix(c(
"$\\Vest{\\overline{y}_{\\pi}}_1$"=SE(svymean(~api00,dclus2.str))^2
,"$\\Vest{\\overline{y}_{\\pi}}_\\text{BRR}$"=SE(svymean(~api00,dclus2.Brr))^2
,"$\\Vest{\\overline{y}_{\\pi}}_\\text{GBRR}$"=SE(svymean(~api00,dclus2.FayBrr))^2
,"$\\Vest{\\overline{y}_{\\pi}}_\\text{d1JK,StrRS}$"=SE(svymean(~api00,dclus2.Jkn))^2
,"$\\Vest{\\overline{y}_{\\pi}}_\\text{MCboot}$"=SE(svymean(~api00,dclus2.Boot))^2
,"$\\Vest{\\overline{y}_{\\pi}}_\\text{subMCboot}$"=SE(svymean(~api00,dclus2.Subboot))^2
,"$\\Vest{\\overline{y}_{\\pi}}_\\text{mrbMCboot}$"=SE(svymean(~api00,dclus2.Mrbboot))^2
))
colnames(Vest.resam) <- "api00"
@
\end{lrbox}
\onslide*<4>{ Resampling weights based on the our design weights
\usebox{\mysavebox} }
\onslide*<5>{ The variance estimates obtained from the resampling methods and the direct method.
<<resamTabVest,echo=FALSE,results='asis'>>=
#give true benchmark?
tab.Vest.resam   <- xtable((Vest.resam),digits = 2,caption = "Estimated Variances for Different Resampling Methods")
align(tab.Vest.resam) <- "r|l"
print(tab.Vest.resam,caption.placement = "top"
      ,sanitize.text.function=identity
      ,add.to.row=list(pos=list(-1),command=c("Estimator"))
      ,hline.after=c(0))
@
}
\begin{lrbox}{\mysavebox}
<<resamSvyObsCalib>>=
svymean(~api99,cal.dclus2.str)
mrbclus2<-as.svrepdesign(cal.dclus2.str, type="mrb",replicates=99)
#does not re-calibrate after each resample!
svymean(~api99,mrbclus2)
@
\end{lrbox}
\onslide*<6>{If you want to use calibrate weights with resampling, then every set of replicate weights (based on the design weights!) has to be calibrated. \usebox{\mysavebox} }

\begin{lrbox}{\mysavebox}
<<resamSvySim,eval=FALSE,size='tiny'>>=
#Be carefull with the numer of runs 'R', this takes a while!
Ests <-
c( "Pest_d" ,"Vest_d" ,"Vest_BRR" ,"Vest_GBRR" ,"Vest_d1JK.StrRS","Vest_MCboot" 
   ,"Vest_subMCboot" ,"Vest_mrbMCboot")
R<-1000
simOUT <- vapply(1:R,function(x){
  #1. stage
  s_I           <- strat.sample(sind=apipop_d$sind,n.h=n_hI)==1
  apiclus1.str  <- apipop.[apipop.$dnum%in%apipop_d[s_I,"dnum"],]
  
  N_i <- n_i   <- table(apiclus1.str$dnum)
  n_i[N_i<3]   <- N_i[N_i<3] 
  n_i[N_i>=3]  <- 2          
  
  #2. stage
  s_i <- strat.sample(apiclus1.str$dnum,n.h=n_i)==1
  apiclus2.str <- apipop.[apipop.$snum%in%apiclus1.str[s_i,"snum"],] 
  
  #define the 'svydesign' object
  dclus2.str     <- svydesign( id = ~dnum + snum, strata=~sind,fpc = ~fpc1 + fpc2, data = apiclus2.str)
  dclus2.Brr     <- as.svrepdesign(dclus2.str, type="BRR")
  dclus2.FayBrr  <- as.svrepdesign(dclus2.str, type="Fay",fay.rho=0.3)
  dclus2.Jkn     <- as.svrepdesign(dclus2.str, type="JKn")
  dclus2.Boot    <- as.svrepdesign(dclus2.str, type="bootstrap",replicates=99)
  dclus2.Subboot <- as.svrepdesign(dclus2.str, type="subbootstrap",replicates=99)
  dclus2.Mrbboot <- as.svrepdesign(dclus2.str, type="mrbbootstrap",replicates=99)
  
  c("Pest_"=svymean(~api00,dclus2.str),"Vest_d"=SE(svymean(~api00,dclus2.str))^2
  ,"Vest_BRR"=SE(svymean(~api00,dclus2.Brr))^2,"Vest_GBRR"=SE(svymean(~api00,dclus2.FayBrr))^2
  ,"Vest_d1JK.StrRS"=SE(svymean(~api00,dclus2.Jkn))^2,"Vest_MCboot"=SE(svymean(~api00,dclus2.Boot))^2
  ,"Vest_subMCboot"=SE(svymean(~api00,dclus2.Subboot))^2,"Vest_mrbMCboot"=SE(svymean(~api00,dclus2.Mrbboot))^2)
 }
 ,FUN.VALUE = array(0,c(1,length(Ests)))
)
dimnames(simOUT) <- list(stat="mean.api00",est=Ests ,sim=1:R)
@
\end{lrbox}
\onslide*<7>{Run a simulation to see which method is best to estimate the variance.
\usebox{\mysavebox} }
\end{frame}

\begin{frame}{Resampling with \\ Imputation and Calibration}
Resampling can be used for variance estimation if a sample has been imputed for item non-response and calibrated as a treatment for unit non-response.
The procedure can be described as follows:
 \begin{itemize}
\item[1]<2-> resample your imputed sample 
\item[2]<3-> impute the resample using the same imputation routing that was used to impute the original sample now based on the resample
\item[3]<4-> calibrate the replicate weights with the same calibration method used to calibrate the original sample
\item[4]<5-> calculate your estimator
\item[5]<6-> repeat this process for each resample
\end{itemize}
\uncover<7>{Then you use the variance estimator that is specific to the resampling method you applied.}
\end{frame}


% Comparison of Methods
%  Replication methods require:
%  Weights
%  Additional replicate weight variables
% ÙÄÅ∏ OR cluster & stratum IDs so users can do
% replication
%  BRR requires 2 PSUs/stratum
% Linearization requires:
%  Weights , Cluster & stratum IDs
%  Disclosure risk in release of these variables?

%The boostrap is the most robous method with repsect to estimators
%An is as flexibale the sampling design as the Jackknife methods.

 \begin{frame}[allowframebreaks]\frametitle{Literature}    
% %\scriptsize
  \begin{thebibliography}{10}
   \setbeamertemplate{bibliography item}[article]
  \bibitem{Fay1989}
   R.E.~Fay.
   \newblock  Theory and Application of Replicate Weighting for Variance Calculations.
   \newblock {\em Survey Research Methods Section: ASA}, 1989.
   \setbeamertemplate{bibliography item}[article]
  \bibitem{Kott2001}
    P.S.~Kott
    \newblock  The delete-a-group Jackknife.
    \newblock {\em Journal of Official Statistics}, 2001.
  \setbeamertemplate{bibliography item}[book]
  \bibitem{ShaoTu1995}
   J.~Shao, D.~Tu.
  \newblock  The Jackknife and Bootstrap.
    \newblock {\em Springer}, 1995.
     \setbeamertemplate{bibliography item}[articel]
     \bibitem{Preston2009}
      J.~Preston.
      \newblock Rescaled bootstrap for stratified multistage sampling.
      \newblock {\em Survey Methodology}, 2009.
   \end{thebibliography}
\end{frame} 





\end{document}
